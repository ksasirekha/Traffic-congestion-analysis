{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: [road_width, traffic_volume, avg_speed]\n",
    "data = [\n",
    "    [10, 500, 60],\n",
    "    [8, 1000, 40],\n",
    "    [6, 1500, 20],\n",
    "    [12, 400, 70],\n",
    "    [7, 1200, 30],\n",
    "    [5, 1800, 15]\n",
    "]\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = [row[:2] for row in data]  # Features: [road_width, traffic_volume]\n",
    "y = [row[2] for row in data]  # Target: [avg_speed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    return [(x - min_val) / (max_val - min_val) for x in data], min_val, max_val\n",
    "\n",
    "# Normalize features and target\n",
    "X_normalized = []\n",
    "scales = []  # To store min-max scales for de-normalization\n",
    "for i in range(len(X[0])):\n",
    "    column = [row[i] for row in X]\n",
    "    norm_col, min_val, max_val = normalize(column)\n",
    "    X_normalized.append(norm_col)\n",
    "    scales.append((min_val, max_val))\n",
    "\n",
    "# Transpose normalized features\n",
    "X_normalized = list(zip(*X_normalized))\n",
    "y_normalized, y_min, y_max = normalize(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, epochs=1000, learning_rate=0.01):\n",
    "    # Initialize weights and bias\n",
    "    weights = [0.0] * len(X[0])  # One weight per feature\n",
    "    bias = 0.0\n",
    "\n",
    "    # Gradient Descent\n",
    "    for epoch in range(epochs):\n",
    "        total_error = 0\n",
    "        for i in range(len(X)):\n",
    "            # Predict using current weights and bias\n",
    "            y_pred = sum([weights[j] * X[i][j] for j in range(len(X[0]))]) + bias\n",
    "            error = y_pred - y[i]\n",
    "\n",
    "            # Update weights and bias using gradient descent\n",
    "            for j in range(len(weights)):\n",
    "                weights[j] -= learning_rate * error * X[i][j]\n",
    "            bias -= learning_rate * error\n",
    "\n",
    "            # Accumulate error\n",
    "            total_error += error ** 2\n",
    "        \n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_error / len(X):.4f}\")\n",
    "\n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3150\n",
      "Epoch 100, Loss: 0.0086\n",
      "Epoch 200, Loss: 0.0014\n",
      "Epoch 300, Loss: 0.0011\n",
      "Epoch 400, Loss: 0.0010\n",
      "Epoch 500, Loss: 0.0010\n",
      "Epoch 600, Loss: 0.0010\n",
      "Epoch 700, Loss: 0.0010\n",
      "Epoch 800, Loss: 0.0010\n",
      "Epoch 900, Loss: 0.0010\n",
      "Trained Weights: [0.6709427546164409, -0.3831057381730891]\n",
      "Trained Bias: 0.33443083919946154\n"
     ]
    }
   ],
   "source": [
    "# Train the linear regression model\n",
    "weights, bias = linear_regression(X_normalized, y_normalized)\n",
    "\n",
    "print(\"Trained Weights:\", weights)\n",
    "print(\"Trained Bias:\", bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Average Speed for [9, 1100]: 43.95 km/h\n"
     ]
    }
   ],
   "source": [
    "def predict(new_point, weights, bias, scales, y_min, y_max):\n",
    "    # Normalize the input point using scales\n",
    "    norm_point = [(new_point[i] - scales[i][0]) / (scales[i][1] - scales[i][0]) for i in range(len(new_point))]\n",
    "\n",
    "    # Compute normalized prediction\n",
    "    y_pred_norm = sum([weights[i] * norm_point[i] for i in range(len(weights))]) + bias\n",
    "\n",
    "    # De-normalize the prediction\n",
    "    y_pred = y_pred_norm * (y_max - y_min) + y_min\n",
    "    return y_pred\n",
    "\n",
    "# Test with a new data point\n",
    "new_point = [9, 1100] \n",
    "predicted_speed = predict(new_point, weights, bias, scales, y_min, y_max)\n",
    "print(f\"Predicted Average Speed for {new_point}: {predicted_speed:.2f} km/h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 3.07\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return sum((yt - yp) ** 2 for yt, yp in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "\n",
    "predictions = [predict(X[i], weights, bias, scales, y_min, y_max) for i in range(len(X))]\n",
    "mse = mean_squared_error(y, predictions)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
